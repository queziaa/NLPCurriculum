## 简介
相对于一般的文本分类问题（情感分析、垃圾邮件识别等），句子关系判别领域的任务更加复杂，因为句子关系判别任务需要考虑句子之间的关联性，而不仅仅是句子本身的特征。句子关系判别任务的目标是判断两个句子之间的关系，比如判断两个句子是否是同义句、是否是因果关系、是否是蕴涵关系等。句子关系判别任务在自然语言处理领域有着广泛的应用，比如问答系统、信息检索、对话系统等。句子关系判别任务的难点在于句子之间的关系多种多样，需要考虑句子之间的语义、逻辑关系，而这些关系往往是隐含的，需要通过模型来学习。句子关系判别任务的关键是设计合适的特征表示和模型，以便捕捉句子之间的关系。句子关系判别任务的研究首先可以分为两个不同的领域，一个是基于语义的句子关系判别，另一个是基于领域知识的句子关系判别。基于语义的句子关系判别主要是通过学习句子之间的语义关系来判断句子之间的关系，然后通过模型来学习句子之间的关系。而基于领域知识的句子关系判别主要是通过领域知识来判断句子之间的关系，比如通过知识图谱、实体关系等方式来表示句子，然后通过模型来学习句子之间的关系。

## 1基于匹配度的语义关系判别
此项任务是一个历史悠久的任务，其目标是判断两个句子之间的语义相似度。传统的方式是通过词频和词共现等基于概率的方式来计算两个句子之间的相似度，但是其无法捕捉句子之间的语义关系。近年来，随着深度学习的发展，基于匹配度的语义关系判别任务得到了很大的发展，主要是参考注意力机制和transformer模型，去学习句子之间的深层语义关系。
对于文本对$(s_1, s_2)$，基于匹配度的语义关系判别任务是判断两个句子之间的语义相似度grade，即$grade = f(s_1, s_2)$，，其在[0, 1]之间，输出grade的值表示两个句子之间的语义相似度，换言之表示二者是否能被互相替换。

### 1.1 LET: Linguistic Knowledge Enhanced Graph Transformer for Chinese Short Text Matching
PropSeg (Propositional Segmentations for Text Similarity)：
引入多个分词工具对句子进行分词，保留不同分词路径，形成词格图。词格图能够捕捉多种可能的词边界信息，增强了模型对词汇层面多样性的处理能力。再附加HowNet获取每个词的词义信息。通过引入外部知识库，模型可以更好地理解和处理中文文本中的词义和同义词关系，提升文本匹配的效果。
之后通过MD-GAT能够处理来自不同维度的信息（如词义、上下文等），在图结构中更新节点的表示。这种多维度的表示学习使模型能够更好地捕捉词汇的多义性和上下文关系。结合了来自HowNet的词义信息和上下文信息，通过多维度的注意力机制来更新节点的表示。其图结构上对节点进行多维度的注意力加权聚合，从而获得更丰富的节点表示。


   对于每个节点 \( x_i \)，首先计算其与邻居节点 \( x_j \) 之间的注意力权重。注意力权重考虑了不同的维度（例如词义和上下文）。注意力权重 \( \alpha_{ij}^{(l,m)} \) 可以通过如下公式计算：

   \[
   \alpha_{ij}^{(l,m)} = \frac{\exp\left( \text{LeakyReLU}\left( \vec{a}_m^\top \left[ \mathbf{W}_m h_l^i \| \mathbf{W}_m h_l^j \right] \right) \right)}{\sum_{k \in N^+(i)} \exp\left( \text{LeakyReLU}\left( \vec{a}_m^\top \left[ \mathbf{W}_m h_l^i \| \mathbf{W}_m h_l^k \right] \right) \right)}
   \]

   其中，\( \| \) 表示向量连接操作，\( \mathbf{W}_m \) 是用于第 \( m \) 个维度的线性变换矩阵，\( \vec{a}_m \) 是用于第 \( m \) 个维度的注意力向量，\( \text{LeakyReLU} \) 是带泄露的ReLU激活函数。


   每个节点 \( x_i \) 的表示通过对其邻居节点的表示进行加权平均来更新，公式如下：

   \[
   h_{l+1}^i = \sigma \left( \sum_{m=1}^M \sum_{j \in N^+(i)} \alpha_{ij}^{(l,m)} \mathbf{W}_m h_l^j \right)
   \]


1. 输入表示：
   每个节点 \( x_i \) 的初始表示为 \( h_0^i \)，其来自预训练语言模型（如BERT）的字符级表示。

2. 多维度特征提取：
   使用多维度的注意力机制，从HowNet获取每个词的词义信息，并结合上下文信息。

3. 注意力加权聚合：
   对每个节点，计算其与邻居节点的多维度注意力权重，通过加权聚合得到更新后的节点表示。

4. 节点表示更新：
   通过多个迭代步骤，不断更新每个节点的表示，使其能够更好地捕捉词汇的多义性和上下文关系。

通过MD-GAT模块，模型可以在图结构中更有效地利用多维度的信息，提升中文短文本匹配的性能。






语义感知图变换器（Semantic-aware Graph Transformer, SaGT）


然后是SaGT层(参见第4.2节)，它融合了单词和语义之间的信息。在每一层中，我们首先更新节点的意义表示，然后使用MD-GAT更新词表示。至于句子匹配层(参见第4.3节)，我们将单词表示转换为字符级别，并在文本之间共享消息。此外，LET可以与预训练的语言模型相结合，例如BERT (Devlin et al. 2019)。它可以看作是在微调阶段将词和语义信息整合到预训练的语言模型中的一种方法。这项工作的贡献总结如下:a)我们支持提出一种新的增强图形转换器，使用语言知识来调节单词歧义。b)对两个中文数据集的实证研究表明，我们的模型不仅优于典型的文本匹配模型，而且优于预训练模型BERT以及BERT的一些变体。c)我们制定策略，语义信息和多粒度信息对于文本匹配建模都很重要，特别是对于较短的文本




提出了SaGT，将词表示和词义表示进行迭代更新，并融合彼此的信息。SaGT能够在每一层中同时更新词节点和词义节点的表示，通过这种交互机制，模型能够更好地捕捉词汇和语义的复杂关系。
在句子匹配层，模型使用双向多视角匹配机制，对两个字符序列进行匹配。这种机制能够从多个角度匹配句子对，进一步增强模型对句子间相似度的捕捉能力。

语义感知图变换器 (SaGT)

SaGT（Semantic-aware Graph Transformer）是一种用于增强中文短文本匹配任务的模型模块。其核心思想是通过迭代更新词和词义的表示，融合上下文信息和语言学知识，以提高模型的匹配性能。


**初始化表示**：
   - 每个词 \( w_i \) 的初始表示为 \( h_0^i \)，来自预训练语言模型（如BERT）。
   - 每个词义 \( s_{i,k} \) 的初始表示为 \( g_0^{i,k} \)，来自HowNet。

**词义表示更新**：
   - 在第 \( l \) 次迭代中，首先更新词义表示 \( g_{l-1}^{i,k} \)。
   - 对于每个词义，从前后方向的邻居节点中聚合信息：

     \[
     m_{i,k}^{l,\text{fw}} = \text{MD-GAT} \left( g_{l-1}^{i,k}, \left\{ h_{l-1}^j \mid x_j \in N_{\text{fw}}^+(x_i) \right\} \right)
     \]

     \[
     m_{i,k}^{l,\text{bw}} = \text{MD-GAT} \left( g_{l-1}^{i,k}, \left\{ h_{l-1}^j \mid x_j \in N_{\text{bw}}^+(x_i) \right\} \right)
     \]

   - 合并前后方向的信息：

     \[
     m_{i,k}^l = [m_{i,k}^{l,\text{fw}}, m_{i,k}^{l,\text{bw}}]
     \]

   - 使用GRU更新词义表示，以控制上下文信息和语义信息的融合：

     \[
     g_l^{i,k} = \text{GRU} \left( g_{l-1}^{i,k}, m_{i,k}^l \right)
     \]

**词表示更新**：
   - 词表示基于更新后的词义表示 \( g_l^{i,k} \) 进行更新。
   - 从词义表示中获取语义信息：

     \[
     q_l^i = \text{MD-GAT} \left( h_{l-1}^i, \left\{ g_l^{i,k} \mid s_{i,k} \in S(w_i) \right\} \right)
     \]

   - 使用GRU更新词表示：

     \[
     h_l^i = \text{GRU} \left( h_{l-1}^i, q_l^i \right)
     \]


- **融合多源信息**：SaGT结合了来自BERT的上下文信息和HowNet的词义信息，通过多维度注意力机制，使得模型能够更好地捕捉词汇的多义性和上下文关系。
- **迭代更新机制**：通过迭代更新词和词义的表示，逐步融合上下文信息和语义信息，提升了模型的匹配准确性。
- **多维度注意力机制**：MD-GAT在图结构上对节点进行多维度的注意力加权聚合，从而获得更丰富的节点表示，增强了模型对复杂语言现象的处理能力。

通过以上步骤，SaGT能够有效地融合上下文和语义信息，提高中文短文本匹配任务的性能。


使用分词工具对输入句子对进行分词，形成词格图。
通过BERT模型获取字符级别的上下文表示，并通过注意力池化方法聚合为词级别表示。
使用MD-GAT从HowNet获取词义表示，并在SaGT层中迭代更新词表示和词义表示。
在句子匹配层中，将词表示转换为字符级别，并匹配两个字符序列。
最后通过关系分类器预测两个句子的相似度。


参考文献：Wang, Y., & Cho, K. (2021). Propositional Segmentations for Improving Text Similarity. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics.



### 1.2 Distilling Knowledge from Reader to Retriever for Question Answering
其中采用CO-DR通过对比蒸馏方法，将大规模预训练模型的知识迁移到轻量级模型中，从而提升了语义相似度任务的效率和效果。


CO-DR 的设计目的是通过利用知识蒸馏技术，从一个强大的阅读器模型中学习到有效的检索信号，从而增强检索器的性能。其核心思想是让检索器学习到如何像阅读器一样评估段落的相关性，而无需人工标注的训练数据。使用一个强大的序列到序列模型（如T5或BART）作为阅读器，来处理多个文档并生成答案。
在阅读器模型中，通过对不同段落的注意力分数，来衡量各段落的相关性。使用阅读器模型的注意力分数作为合成标签，这些分数反映了各段落对于生成最终答案的重要性。合成标签用于指导检索器的训练，使得检索器能够学习到如何评估段落的相关性。
密集双编码器检索模型（Dense Bi-Encoder Retriever Model）

Dense Bi-Encoder 是一种用于信息检索任务的双编码器模型，它使用密集表示（dense representation）来对查询（query）和文档（document）进行编码，并通过计算它们的相似度来检索相关文档。具体来说，Dense Bi-Encoder 模型包括两个独立的编码器：一个用于编码查询，另一个用于编码文档。以下是对 Dense Bi-Encoder 的详细解释：

- **查询编码器（Query Encoder）** 和 **文档编码器（Document Encoder）**：这两个编码器通常使用预训练的语言模型（如BERT）来生成查询和文档的表示。
- **编码过程**：查询和文档分别被编码为固定维度的向量。具体来说，给定一个查询 \( q \) 和一个文档 \( d \)，查询编码器 \( E_q \) 将查询 \( q \) 编码为向量 \( E_q(q) \)，文档编码器 \( E_d \) 将文档 \( d \) 编码为向量 \( E_d(d) \)。

- **向量内积（Dot Product）**：查询向量和文档向量的相似度通过计算它们的内积来衡量。即，相似度 \( S(q, d) \) 可以表示为：
  \[
  S(q, d) = E_q(q) \cdot E_d(d)
  \]
  内积值越大，表示查询和文档之间的相似度越高。

- **高效检索**：在实际应用中，检索过程通常涉及从大量文档中找到与给定查询最相似的文档。为此，可以使用高效的相似度搜索库（如FAISS）来快速进行最近邻搜索。
- **预计算文档向量**：在检索过程中，文档向量通常是预先计算并存储好的，这样在进行查询时，只需对查询进行编码并与预计算的文档向量进行快速比较即可。

- **训练目标**：模型的训练目标是使相关文档的相似度得分高于不相关文档。为此，通常使用对比损失函数（如最大边距损失）来训练模型。
- **负样本选择**：为了有效训练，模型需要选择合适的负样本（即不相关的文档）。负样本可以通过随机选择或硬负样本挖掘来获取。

- **高效性**：Dense Bi-Encoder 通过向量化表示和快速内积计算，实现了高效的相似度计算和检索过程。
- **可扩展性**：由于文档向量可以预先计算并存储，Dense Bi-Encoder 能够在大规模数据集上进行高效检索。
- **语义丰富性**：利用预训练语言模型，Dense Bi-Encoder 可以捕捉查询和文档中的丰富语义信息，提高检索效果。

1. **查询输入**：用户输入一个查询 \( q \)。
2. **查询编码**：查询编码器将查询 \( q \) 编码为向量 \( E_q(q) \)。
3. **文档检索**：使用预先计算好的文档向量集合，通过相似度计算找到与查询最相似的文档。
4. **返回结果**：系统返回最相关的文档作为检索结果。

通过以上解释，我们可以看出，Dense Bi-Encoder 是一种高效且强大的检索模型，它利用双编码器结构和密集表示来实现快速准确的文档检索。




参考文献：Izacard, G., & Grave, E. (2021). . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.

## 2基于关系的语义关系判别
相对于基于匹配度的任务，基于关系的语义关系判别任务更加复杂，要求细粒度的判断两个句子之间的关系。基于关系的语义关系判别任务的目标是判断两个句子之间的关系，比如判断两个句子是否是同义句、是否是因果关系、是否是蕴涵关系等。基于关系的语义关系判别任务的难点在于句子之间的关系多种多样，需要考虑句子之间的语义、逻辑关系，而这些关系往往是隐含的，需要通过模型来学习。基于关系的语义关系判别任务的关键是设计合适的特征表示和模型，以便捕捉句子之间的关系。
此项任务与文本推理任务有很大的相似性，但是文本推理任务更加注重逻辑关系，而基于关系的语义关系判别任务更加注重语义关系。对于文本对$(s_1, s_2)$，基于关系的语义关系判别任务是判断两个句子之间的关系label，即$label = f(s_1, s_2)$，label的取值范围是一个有限集合，表示两个句子之间的关系，比如同义关系、因果关系、蕴涵关系等。

### 2.1 Description-Enhanced Label Embedding Contrastive Learning for Text Classification


提出了一种新颖的关系学习网络（R2-Net），以简单而有效的方式充分利用标签。首先利用预训练语言模型编码器和基于卷积神经网络的编码器分别对输入单词和句子的全局和部分语义含义进行建模。然后，受自监督学习的启发，提出了一个自监督关系分类任务来增强R2-Net的学习能力，使输入文本之间的类间关系得到全面建模（不同标签关系之间的差异）。此外，使用三元损失来衡量相同类别之间的连接（具有相同标签的输入将表示得更接近，反之则表示得更远）。然后，利用一个MLP来预测相应的标签。
其中重塑了任务的目标函数，将原有的输入句子对判断其关系，改为输入两个句子对X1和X2，目标是学习一个分类函数F，用于确定这两个输入文本是否具有相同的语义关系作者称呼为R2分类。这样做的目的是通过句子对产生的两个向量进行乘积、做差等操作捕捉内部差异，得到关系向量特征，在对两个句子对的关系向量特征进行学习，从而完成关系判断任务且捕捉到句子对之间的关系。
除了利用R2分类任务来学习成对关系信息外，从基于三元组的关系中学习类内和类间信息。因此，将三元组损失（triplet loss）引入到R2-Net中。作为一种基本的相似性函数，三元组损失在信息检索领域被广泛应用，能够减小具有相同关系的输入对之间的距离，	增大具有不同关系的输入对之间的距离。因此，首先在这个模块中计算距离。这个组件的输入是三个语义表示为负对。为了获得更好的结果，首先将它们通过全连接层转换到一个公共空间中，然后分别计算锚定对和正对之间的距离以及锚定对和负对之间的距离。

### 2.2  Knowledge-enhanced prototypical network with class cluster loss for few-shot elation classification

本文介绍的模型是一个用于少样本关系分类的知识增强原型网络，首先使用BERT模型数据集中的实例进行嵌入。
系标签和描述信息也被输入到句子编码器中，生成关系特征向量生成的句子特征向量包括全局表示和局部表示，之后通过原型增强模块。原型网络通过计算查询实例与类原型之间的度量空间距离进行分类。初始原型是通过数据集中实例的平均值计算得到的。引入关系标签和描述作为补充数据，增强原型的表示能力。
   - 使用公式计算每个关系类的基本原型：
     \[
     P_{n}^{\text{bas}} = \frac{1}{K} \sum_{k=1}^{K} f_{\theta}(s_{n,k})
     \]
   - 其中，\(f_{\theta}()\)表示句子编码器。

   - 引入类聚类损失函数，促使模型学习到一个具有高度辨别力的度量空间。
   - 这个损失函数鼓励类内紧凑性和类间分离性。




原型增强模块

原型网络通过计算查询实例与类原型之间的度量空间距离进行分类。通常，原型是从支持实例的平均值中得出的，这缺乏可靠的先验知识。外部知识（例如关系标签和描述）可以为关系分类任务提供强有力的支持证据，这些信息对于少样本关系分类任务是现成可用的。
遵循原型网络的典型配置，句子编码器对N类中的K个实例进行编码。对于每个关系，K个实例的平均值被用作基本原型P<sub>bas</sub>，其计算公式如下：
\[ P_{n}^{\text{bas}} = \frac{1}{K} \sum_{k=1}^{K} f_{\theta}(s_{n,k}) \]
其中，\( f_{\theta}() \) 表示句子编码器。

 知识增强机制示例
1. **原始原型分布**：查询实例q最接近原型p<sub>a</sub>。
2. **通过原型增强模块修改的原型分布**：r代表关系信息。
3. **更新的原型分布**：查询实例q最接近原型p<sub>b</sub>，现在正确分类。

 4.2.2 增强原型的计算
通过直接相加来实现增强原型的计算：
\[ P_{n}^{\text{enh}} = R_{n}^{\text{rep}} \cdot G + P_{n}^{\text{bas}} \]
其中，n = 1, ..., N; \( P_{n}^{\text{enh}} \in R^{2d} \)。上面使用的所有表示都属于 \( R^{2d} \)。

 4.3 类聚类损失

FSRC模型的性能在很大程度上依赖于度量空间中实例向量的分布。为了学习一个具有高区分度的度量空间，设计了一种类聚类损失。正如图4所示，类聚类损失的主要目标是限制类内紧凑性和类间可分离性。

 4.3.1 三元组损失
我们的工作基于三元组损失进行了改进。三元组损失的目的是区分不同类别的样本，可以定义为：
\[ L_{\text{tri}} = \text{relu}(D_{a,p} - D_{a,n} + m) \]
其中，\( D_{a,p} \) 和 \( D_{a,n} \) 分别表示样本a与正样本p和负样本n之间的距离，m是一个边界值，用于确保正负样本之间的距离差异足够大。




## 3基于领域知识的语义关系判别
相对于之前的两个任务，这个任务最大的区别是输入不再是两个句子，而是一个句子和一个实体。基于领域知识的语义关系判别任务的目标是判断一个句子和一个实体之间的关系，比如判断一个句子是否是一个实体的属性、是否是一个实体的同义词等。基于领域知识的语义关系判别任务的难点在于需要考虑句子和实体之间的关系，而这种关系往往是隐含的，需要通过模型来学习。基于领域知识的语义关系判别任务的关键是设计合适的特征表示和模型，以便捕捉句子和实体之间的关系。
对于任务设定上，这一任务常被定义为多分类任务，或者是标签预测任务，这两者是等价的。之后的解释中以标签任务称呼这一任务。对于文本对$(s, e)$，基于领域知识的语义关系判别任务是判断一个句子和一个实体之间的关系label，即$label = f(s, e)$，label的取值范围是一个预先给定的有限集合，常使用专业领域的名词（主体）来表示。
在这一任务的研究中，常常会引入外部知识，比如知识图谱、实体关系等，通过外部知识，模型可以更好地学习句子和实体之间的关系。
如果考虑标签本身是知识图谱的实体，那么这一任务进一步将分类结果映射到知识图谱的实体上，得到拥有结果关系的分类结果，例如分层文本分类任务。

### 3.1 Concept-Based Label Embedding via Dynamic Routing for Hierarchical Text Classification
层次文本分类在分类层次结构中对文本进行分类。常使用外部字典来建模类的表示。文本采用类之间共享的概念，作为特定领域的细粒度信息建模标签，对类之间的共享机制进行建模，用于分层文本分类。
文中通过胶囊网络中的动态路由概念，将概念连接至标签，通过对连接的权重，将概念信息组合作为标签的表示。这种方法能够更好地捕捉类之间的关系。之后对通过并联三个注意力机制（每个注意力均是Q为文档，K为标签信息）之后再加入分类器作为一个CCM，作为某一层的标签分类器。再串联多个CCM完成整个分类任务。


### 3.2 Follow the Path: Hierarchy-Aware Extreme Multi-Label Completion for Semantic Text Tagging

可以看到模型的架构基本就是Transformer，这里以模型开始解释论文，这里论文将大量标签的标记任务或者说是分层分类问题通过Transformer完成，作者通过Encoder学习文档信息，Decoder学习标签信息的方式来完成分类问题，相对于之前的一些任务的模型这里可以说是一个很是适当的方式，文档信息和标签信息均得到了足够的学习，双方的连接也是合理，同时得益于Transformer的序列输出特点，这里将树状的标签信息视为多条序列，使得每次输出可以聚焦到一个较小的范围，对于其他一些相关的论文最后进行标签输出是一个很困难的任务，常常使用多个分类器。同时因为Transformer的特性，可以预想到这个有着良好的扩展性。在作者的实验中也展示了在较大规模数据集上的实验结果。


编码器自注意力：基于整个输入序列编码每个词的上下文。编码器自注意力学习输入文档中令牌的上下文化嵌入；
解码器自注意力：考虑之前生成的令牌对当前令牌生成步骤的影响。编码器-解码器交叉注意力捕捉输入令牌和输出标签之间的细粒度依赖关系；
编码器-解码器交叉注意力：在生成过程中聚焦于编码器输出的相关部分。解码器自注意力考虑之前预测的标签以生成标签树中的连贯路径。






## 4外部知识
对于以上任务，目前的研究常通过引入外部知识来提升模型的性能。外部知识可以是知识图谱、实体关系等，通过外部知识，模型可以更好地学习句子之间的关系。外部知识的引入可以通过两种方式，一种是将外部知识作为特征输入到模型中，另一种是将外部知识作为模型的先验知识，通过模型来学习句子之间的关系。外部知识的引入可以提升模型的性能，但是也会增加模型的复杂度，需要更多的计算资源。
### 4.1知识图谱
知识图谱是一种用于表示实体之间关系的图结构，其中实体表示为节点，关系表示为边。知识图谱可以用于表示实体之间的关系，比如人物之间的关系、实体之间的关系等。知识图谱可以通过图卷积网络（GCN）等模型来学习实体之间的关系，然后通过模型来学习句子之间的关系。
常用于增强模型的性能，通过知识图谱，模型可以更好地学习句子之间的关系，尤其是专业领域的专用词之间的关系。
### 4.2实体关系
这里是指只有实体之间的关系网络，通过标记实体的属性、关系等，可以用于补充模型的训练数据。对于分层文本分类问题，实体关系只保留上下级关系（层次关系，is-a关系），通过树状的实体关系作为标签，可以将文档分类为层次关系。
### 4.3


## 实验

Title：Follow the Path: Hierarchy-Aware Extreme Multi-Label Completion for Semantic Text Tagging
Publication：WWW '24: Proceedings of the ACM on Web Conference 2024
Github: https://github.com/eXascaleInfolab/HECTOR
近年来，极端多标签（XML）问题，特别是XML补全任务——预测实体缺失标签的任务——已经引起了广泛关注。大多数XML补全问题可以自然地利用标签层次结构，该层次结构可以表示为编码不同标签之间关系的树结构。 在本文中，我们提出了一种新的算法，HECTOR——基于变换器的文本极端层次补全算法，用以更有效地解决XML补全问题。HECTOR通过直接预测标签树中的路径而非单个标签来运作，从而利用层次结构中编码的信息。由于这些路径的序列性，HECTOR能够利用变换器架构的效能和性能，超越现有的XML补全方法。在三个真实世界数据集上的广泛评估显示了我们方法对于XML补全的有效性。我们将HECTOR与几种现有的XML补全方法进行了比较，特别是在标签精炼场景中，即只观察到分类法中前几个顶级层次的粗略标签的情况。在三个不同数据集上的实验结果表明，我们的方法显著优于现有技术，HECTOR频繁地根据多个指标超过之前技术10%以上。
 
	可以看到模型的架构基本就是Transformer，这里以模型开始解释论文，这里论文将大量标签的标记任务或者说是分层分类问题通过Transformer完成，作者通过Encoder学习文档信息，Decoder学习标签信息的方式来完成分类问题，相对于之前的一些任务的模型这里可以说是一个很是适当的方式，文档信息和标签信息均得到了足够的学习，双方的连接也是合理，同时得益于Transformer的序列输出特点，这里将树状的标签信息视为多条序列，使得每次输出可以聚焦到一个较小的范围，对于其他一些相关的论文最后进行标签输出是一个很困难的任务，常常使用多个分类器。同时因为Transformer的特性，可以预想到这个有着良好的扩展性。在作者的实验中也展示了在较大规模数据集上的实验结果。


对其中的三个注意力机制：
编码器自注意力：基于整个输入序列编码每个词的上下文。
解码器自注意力：考虑之前生成的令牌对当前令牌生成步骤的影响。
编码器-解码器交叉注意力：在生成过程中聚焦于编码器输出的相关部分。
在我们的任务上下文中，这三种注意力机制执行以下功能：
编码器自注意力学习输入文档中令牌的上下文化嵌入；
编码器-解码器交叉注意力捕捉输入令牌和输出标签之间的细粒度依赖关系；
解码器自注意力考虑之前预测的标签以生成标签树中的连贯路径。


(1) 预处理：完成并重新组合分配给每个文档的正面标签，以在树中形成一组路径。 
(2) 训练：训练一个序列到序列模型，其中文档是输入序列，路径是目标序列。
(3) 推理：给定一个输入文档和一组不完整的标签，解码标签树中的路径。然后将它们合并并根据标签分数排序，生成最终的标签排名，用于预测。 方法的一个额外优势是路径中的标签是按顺序解码的，从最一般的概念到更具体的概念。作者认为这种方法特别适合标签补全，正如我们的实验结果所示。


预测层。解码器生成上下文化的标签表示，这些表示被投影到最终的|𝑉 |-维向量上，其中|𝑉 |是标签词汇表的大小。结果向量的每个元素代表相应标签的概率。预测层由一个全连接层和一个Softmax激活函数组成。
损失函数。遵循原始变换器架构，我们使用Kullback-Leibler散度损失，它衡量两个概率分布之间的不相似性。在训练期间，我们使用值𝜖𝑙𝑠 = 0.2 [27]的标签平滑。标签平滑是一种正则化技术，涉及将目标标签的独热编码替换为平滑分布。标签平滑不是给真实标签分配概率1，而是给其他所有标签分配概率0，而是给真实标签分配一个信心分数，并在其他标签中重新分配平滑质量。在HECTOR中，在损失函数中引入了一些关于标签分类法的先验知识。由于我们的目标是解码树路径而不是非结构化序列，我们提前知道每个位置可以出现哪些标签。因此，在第𝑖个位置，只有分类法中深度为𝑖的层级上的标签可能出现。我们通过应用掩码到标签上来利用这一知识，使平滑质量在相应的层级上重新分配，将所有其他标签的概率设置为0。我们将在第4.3节讨论这种方法的影响。
训练。在多标签问题中，每个文档可以有来自不同（子）领域的标签，结果在标签树中形成多个路径。在训练期间，我们随机选择每个文档的一个路径作为每个训练时期的真实输出。这种方法背后的思想是在训练中引入一些变化性，避免过度拟合特定的输出序列——与[32]的观察一致。通过在训练期间随机选择一个可能的输出路径作为真实输出，模型学会以相同的概率生成所有可能的输出路径。

使用HECTOR进行标签补全 在推理过程中，HECTOR接收包含已知标签的路径前缀。我们使用波束搜索为每个数据点生成多个路径并预测缺失标签。与贪婪搜索不同，贪婪搜索在每一步选择概率最高的候选者，波束搜索维护一组最有前途的候选序列，称为波束。具体来说，算法如下进行：
• 模型为位置𝑖生成一组候选标签。
• 选择概率最高的前𝑘个候选者，其中𝑘是波束宽度。
• 将选定的候选者附加到前面的部分序列（从位置1到𝑖−1的预测标签）上，并计算扩展序列的联合概率。
将前𝑘个扩展序列传递到下一步，为位置𝑖+1生成一组候选标签。






数据集
我们在三个著名的大规模数据集上评估我们的方法：MAG-CS、PubMed 和 EURLex。
• MAG-CS：Microsoft Academic Graph (MAG) 计算机科学 (CS) 是 MAG 数据集的一个子集，专注于计算机科学领域，包含从 1990 年到 2020 年在 105 个顶级 CS 会议上发表的论文，而标签树包含“计算机科学”根级别的相关概念的后代。
• PubMed：我们使用 [36] 发布的 PubMed 子集，其中包括从 2010 年到 2020 年在 150 个顶级医学期刊上发表的论文。每篇 PubMed 论文都用医学主题词表 (MeSH) 中的相关概念进行标注。
• EURLex：EURLex [18] 是最常见的 XMLC 基准数据集之一。它包含来自 EUR-LEX 门户的英文欧盟立法文件，并用欧洲词汇表 (EuroVoc) 中的概念（标签）进行标注。我们使用 在 2019 年发布的最新版本的 EURLex。
我们进一步扩展了每个数据点的标签集，使它们构成树中的完整路径，如第 3.2 节所述。我们在表 1 中报告了数据集的重要统计数据，图 3 总结了 3 个数据集中每个级别的标签分布。
 
 


实验
这个项目是从头训练一个Transformer，encoder300维 decoder600维 均为6层 12个头 ，encoder 序列长300 ，decoder长16，一万条数据 100轮，时间约17小时。

 
实验结构和论文中给出的一致

评价指标的计算方式如下，rank根据正确率返回1 or 0
 


 ## 结论
 

